{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udeb4 Bike-Level Predictive Maintenance Analysis\n",
    "## Team: Anomaly Archers (Section B)\n",
    "\n",
    "**Techniques Used:**\n",
    "- K-Means Clustering (Unsupervised Learning)\n",
    "- Isolation Forest (Anomaly Detection)\n",
    "- Time Series Analysis\n",
    "- Health Scoring & Maintenance Ranking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Enhanced Data\n",
    "\n",
    "We use the **synthesized dataset** with:\n",
    "- `bike_id` - Unique bike identifier\n",
    "- `start_odometer_km` / `end_odometer_km` - Distance meter readings\n",
    "- `user_rating` - Customer feedback (nullable)\n",
    "- `complaint_flag` - Customer complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enhanced dataset with bike IDs\n",
    "df = pd.read_csv('../data/processed/enhanced_trips.csv')\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "\n",
    "print(f\"\u2705 Loaded {len(df):,} trips\")\n",
    "print(f\"   Unique bikes: {df['bike_id'].nunique():,}\")\n",
    "print(f\"   Classic bikes: {(df['rideable_type']=='classic_bike').sum():,} trips\")\n",
    "print(f\"   Electric bikes: {(df['rideable_type']=='electric_bike').sum():,} trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample data\n",
    "df[['ride_id', 'bike_id', 'rideable_type', 'duration_min', \n",
    "    'start_odometer_km', 'end_odometer_km', 'trip_distance_km',\n",
    "    'user_rating', 'complaint_flag']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bike-Level Feature Engineering\n",
    "\n",
    "Aggregate trip data to create **per-bike features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bike-level features\n",
    "bike_features = df.groupby('bike_id').agg(\n",
    "    rideable_type=('rideable_type', 'first'),\n",
    "    total_trips=('ride_id', 'count'),\n",
    "    total_duration_min=('duration_min', 'sum'),\n",
    "    avg_trip_duration=('duration_min', 'mean'),\n",
    "    std_trip_duration=('duration_min', 'std'),\n",
    "    total_distance_km=('trip_distance_km', 'sum'),\n",
    "    avg_trip_distance=('trip_distance_km', 'mean'),\n",
    "    first_trip=('started_at', 'min'),\n",
    "    last_trip=('started_at', 'max'),\n",
    "    unique_start_stations=('start_station_id', 'nunique'),\n",
    "    unique_end_stations=('end_station_id', 'nunique'),\n",
    "    member_trips=('member_casual', lambda x: (x == 'member').sum()),\n",
    "    avg_user_rating=('user_rating', 'mean'),\n",
    "    ratings_given=('user_rating', lambda x: x.notna().sum()),\n",
    "    complaint_count=('complaint_flag', 'sum'),\n",
    "    days_since_service=('days_since_service', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Derived features\n",
    "bike_features['days_active'] = (bike_features['last_trip'] - bike_features['first_trip']).dt.days + 1\n",
    "bike_features['trips_per_day'] = bike_features['total_trips'] / bike_features['days_active']\n",
    "bike_features['member_ratio'] = bike_features['member_trips'] / bike_features['total_trips']\n",
    "bike_features['complaint_rate'] = bike_features['complaint_count'] / bike_features['total_trips']\n",
    "bike_features['short_trip_ratio'] = df.groupby('bike_id').apply(\n",
    "    lambda x: (x['duration_min'] < 3).sum() / len(x)\n",
    ").values\n",
    "\n",
    "# Add cumulative mileage from odometer\n",
    "max_odometer = df.groupby('bike_id')['end_odometer_km'].max().reset_index()\n",
    "max_odometer.columns = ['bike_id', 'cumulative_mileage']\n",
    "bike_features = bike_features.merge(max_odometer, on='bike_id', how='left')\n",
    "\n",
    "# Fill NaN\n",
    "bike_features = bike_features.fillna(0)\n",
    "\n",
    "print(f\"\u2705 Created features for {len(bike_features):,} bikes\")\n",
    "bike_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Features for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering\n",
    "feature_columns = [\n",
    "    'total_trips', 'total_distance_km', 'avg_trip_duration', 'avg_trip_distance',\n",
    "    'trips_per_day', 'member_ratio', 'avg_user_rating', 'complaint_rate',\n",
    "    'short_trip_ratio', 'days_since_service', 'cumulative_mileage'\n",
    "]\n",
    "\n",
    "X = bike_features[feature_columns].copy()\n",
    "X = X.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\u2705 Feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"\\nFeatures used: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means Clustering\n",
    "\n",
    "Group bikes by usage patterns to identify different \"usage profiles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using Elbow Method\n",
    "inertias = []\n",
    "silhouette_scores_list = []\n",
    "K_range = range(2, 8)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores_list.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('K')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method for Optimal K')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(K_range, silhouette_scores_list, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score vs K')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/bike_elbow_silhouette.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSilhouette Scores: {dict(zip(K_range, [f'{s:.3f}' for s in silhouette_scores_list]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with K=4\n",
    "K = 4\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "bike_features['cluster'] = cluster_labels\n",
    "\n",
    "# Name clusters based on usage intensity\n",
    "cluster_summary = bike_features.groupby('cluster')[feature_columns].mean()\n",
    "usage_order = cluster_summary['trips_per_day'].sort_values().index.tolist()\n",
    "\n",
    "cluster_names = {\n",
    "    usage_order[0]: '\ud83d\udfe2 Light Usage',\n",
    "    usage_order[1]: '\ud83d\udfe1 Moderate Usage',\n",
    "    usage_order[2]: '\ud83d\udfe0 Heavy Usage',\n",
    "    usage_order[3]: '\ud83d\udd34 Extreme Usage'\n",
    "}\n",
    "bike_features['cluster_name'] = bike_features['cluster'].map(cluster_names)\n",
    "\n",
    "print(f\"\u2705 Clustering complete (K={K})\")\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for c, name in cluster_names.items():\n",
    "    count = (bike_features['cluster'] == c).sum()\n",
    "    print(f\"  {name}: {count} bikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#2ecc71', '#f1c40f', '#e67e22', '#e74c3c']\n",
    "for cluster_id in sorted(bike_features['cluster'].unique()):\n",
    "    mask = bike_features['cluster'] == cluster_id\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[cluster_id],\n",
    "                label=cluster_names[cluster_id], alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "plt.title('Bike Clusters by Usage Profile (PCA)')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../outputs/figures/bike_cluster_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection (Isolation Forest)\n",
    "\n",
    "Identify bikes with unusual behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42, n_estimators=100)\n",
    "anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
    "anomaly_scores = iso_forest.decision_function(X_scaled)\n",
    "\n",
    "bike_features['anomaly'] = anomaly_labels\n",
    "bike_features['anomaly_score'] = anomaly_scores\n",
    "bike_features['is_anomaly'] = anomaly_labels == -1\n",
    "\n",
    "print(f\"\u2705 Anomaly detection complete\")\n",
    "print(f\"   Normal bikes: {(anomaly_labels == 1).sum()}\")\n",
    "print(f\"   Anomalous bikes: {(anomaly_labels == -1).sum()}\")\n",
    "print(f\"   Anomaly rate: {(anomaly_labels == -1).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomalies\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_pca[~bike_features['is_anomaly'], 0],\n",
    "            X_pca[~bike_features['is_anomaly'], 1],\n",
    "            c='#3498db', label='Normal', alpha=0.5, s=50)\n",
    "plt.scatter(X_pca[bike_features['is_anomaly'], 0],\n",
    "            X_pca[bike_features['is_anomaly'], 1],\n",
    "            c='#e74c3c', label='Anomaly', alpha=0.9, s=120, marker='X', edgecolors='black')\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Bike Anomaly Detection (Isolation Forest)')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../outputs/figures/bike_anomaly_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis\n",
    "\n",
    "Track usage trends to detect degradation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate usage trends per bike\n",
    "df['date'] = df['started_at'].dt.date\n",
    "daily_stats = df.groupby(['bike_id', 'date']).agg(\n",
    "    daily_trips=('ride_id', 'count'),\n",
    "    daily_distance=('trip_distance_km', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "def calculate_trend(group):\n",
    "    if len(group) < 7:\n",
    "        return 0\n",
    "    x = np.arange(len(group))\n",
    "    y = group['daily_trips'].values\n",
    "    slope, _, _, _, _ = stats.linregress(x, y)\n",
    "    return slope\n",
    "\n",
    "bike_trends = daily_stats.groupby('bike_id').apply(calculate_trend).reset_index()\n",
    "bike_trends.columns = ['bike_id', 'usage_trend']\n",
    "\n",
    "# Merge and categorize\n",
    "bike_features = bike_features.merge(bike_trends, on='bike_id', how='left')\n",
    "bike_features['usage_trend'] = bike_features['usage_trend'].fillna(0)\n",
    "\n",
    "bike_features['trend_category'] = pd.cut(\n",
    "    bike_features['usage_trend'],\n",
    "    bins=[-np.inf, -0.3, 0.3, np.inf],\n",
    "    labels=['\ud83d\udcc9 Declining', '\u27a1\ufe0f Stable', '\ud83d\udcc8 Increasing']\n",
    ")\n",
    "\n",
    "print(\"\u2705 Trend analysis complete\")\n",
    "print(\"\\nTrend Distribution:\")\n",
    "print(bike_features['trend_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Health Scoring & Maintenance Ranking\n",
    "\n",
    "Combine all signals into a unified **health score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate health score components\n",
    "\n",
    "# 1. Cluster Risk\n",
    "cluster_risk_map = {usage_order[0]: 0.1, usage_order[1]: 0.3, usage_order[2]: 0.6, usage_order[3]: 0.9}\n",
    "bike_features['cluster_risk'] = bike_features['cluster'].map(cluster_risk_map)\n",
    "\n",
    "# 2. Anomaly Risk (normalized 0-1)\n",
    "min_score = bike_features['anomaly_score'].min()\n",
    "max_score = bike_features['anomaly_score'].max()\n",
    "bike_features['normalized_anomaly'] = 1 - ((bike_features['anomaly_score'] - min_score) / (max_score - min_score))\n",
    "\n",
    "# 3. Trend Risk\n",
    "bike_features['trend_category_str'] = bike_features['trend_category'].astype(str)\n",
    "trend_risk_map = {'\ud83d\udcc9 Declining': 0.7, '\u27a1\ufe0f Stable': 0.3, '\ud83d\udcc8 Increasing': 0.1}\n",
    "bike_features['trend_risk'] = bike_features['trend_category_str'].map(trend_risk_map).fillna(0.3)\n",
    "\n",
    "# 4. Complaint Risk\n",
    "bike_features['complaint_risk'] = (bike_features['complaint_rate'] * 10).clip(0, 1)\n",
    "\n",
    "# 5. Service Age Risk\n",
    "bike_features['service_risk'] = (bike_features['days_since_service'] / 180).clip(0, 1)\n",
    "\n",
    "# 6. Rating Risk\n",
    "bike_features['rating_risk'] = np.where(\n",
    "    bike_features['avg_user_rating'] > 0,\n",
    "    1 - (bike_features['avg_user_rating'] / 5),\n",
    "    0.5\n",
    ")\n",
    "\n",
    "# Composite Score (weighted average)\n",
    "bike_features['health_score'] = (\n",
    "    0.20 * bike_features['cluster_risk'] +\n",
    "    0.20 * bike_features['normalized_anomaly'] +\n",
    "    0.10 * bike_features['trend_risk'] +\n",
    "    0.20 * bike_features['complaint_risk'] +\n",
    "    0.15 * bike_features['service_risk'] +\n",
    "    0.15 * bike_features['rating_risk']\n",
    ")\n",
    "\n",
    "# Categorize\n",
    "def categorize_health(score):\n",
    "    if score < 0.35: return '\ud83d\udfe2 Stable'\n",
    "    elif score < 0.55: return '\ud83d\udfe1 Warning'\n",
    "    else: return '\ud83d\udd34 Critical'\n",
    "\n",
    "bike_features['health_category'] = bike_features['health_score'].apply(categorize_health)\n",
    "\n",
    "print(\"\u2705 Health scoring complete\")\n",
    "print(\"\\nHealth Distribution:\")\n",
    "for cat in ['\ud83d\udfe2 Stable', '\ud83d\udfe1 Warning', '\ud83d\udd34 Critical']:\n",
    "    count = (bike_features['health_category'] == cat).sum()\n",
    "    pct = count / len(bike_features) * 100\n",
    "    print(f\"  {cat}: {count} bikes ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Maintenance Priority Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create priority ranking\n",
    "priority_ranking = bike_features.sort_values('health_score', ascending=False)[\n",
    "    ['bike_id', 'rideable_type', 'cluster_name', 'health_score', 'health_category',\n",
    "     'total_trips', 'cumulative_mileage', 'complaint_count', 'avg_user_rating', 'days_since_service']\n",
    "].reset_index(drop=True)\n",
    "priority_ranking.index = priority_ranking.index + 1\n",
    "priority_ranking.index.name = 'Priority'\n",
    "\n",
    "# Save\n",
    "priority_ranking.to_csv('../outputs/bike_maintenance_priority.csv')\n",
    "\n",
    "print(\"\ud83d\udccb TOP 20 BIKES REQUIRING MAINTENANCE:\")\n",
    "priority_ranking.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "health_counts = bike_features['health_category'].value_counts()\n",
    "color_map = {'\ud83d\udfe2 Stable': '#2ecc71', '\ud83d\udfe1 Warning': '#f1c40f', '\ud83d\udd34 Critical': '#e74c3c'}\n",
    "bar_colors = [color_map.get(c, '#95a5a6') for c in health_counts.index]\n",
    "axes[0].bar(health_counts.index, health_counts.values, color=bar_colors)\n",
    "axes[0].set_xlabel('Health Category')\n",
    "axes[0].set_ylabel('Number of Bikes')\n",
    "axes[0].set_title('Fleet Health Distribution')\n",
    "\n",
    "axes[1].hist(bike_features['health_score'], bins=30, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "axes[1].axvline(x=0.35, color='green', linestyle='--', linewidth=2, label='Stable threshold')\n",
    "axes[1].axvline(x=0.55, color='orange', linestyle='--', linewidth=2, label='Warning threshold')\n",
    "axes[1].set_xlabel('Health Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Health Score Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/bike_health_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 priority bikes\n",
    "top10 = priority_ranking.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_colors = ['#e74c3c' if cat == '\ud83d\udd34 Critical' else '#f1c40f' for cat in top10['health_category']]\n",
    "bars = plt.barh(range(len(top10)), top10['health_score'], color=bar_colors)\n",
    "plt.yticks(range(len(top10)), top10['bike_id'])\n",
    "plt.xlabel('Health Score (Higher = More Urgent)')\n",
    "plt.title('\ud83d\udd27 Top 10 Bikes Requiring Maintenance')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, (score, cat) in enumerate(zip(top10['health_score'], top10['health_category'])):\n",
    "    plt.text(score + 0.01, i, f'{score:.2f} {cat}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/bike_top10_priority.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions & Business Insights\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Clustering**: Identified 4 distinct usage profiles\n",
    "2. **Anomaly Detection**: Found outlier bikes with unusual patterns\n",
    "3. **Time Series**: Tracked usage trends (seasonal decline expected)\n",
    "4. **Health Scoring**: Combined all signals into actionable priority list\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "- **Proactive Maintenance**: Address critical bikes before failures\n",
    "- **Resource Optimization**: Focus repair teams on highest-risk bikes\n",
    "- **Cost Reduction**: Prevent expensive emergency repairs\n",
    "- **Customer Satisfaction**: Fewer breakdowns = happier customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udcca BIKE-LEVEL ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal bikes analyzed: {len(bike_features):,}\")\n",
    "print(f\"  - Classic: {(bike_features['rideable_type']=='classic_bike').sum():,}\")\n",
    "print(f\"  - Electric: {(bike_features['rideable_type']=='electric_bike').sum():,}\")\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for name in sorted(bike_features['cluster_name'].unique()):\n",
    "    count = (bike_features['cluster_name'] == name).sum()\n",
    "    print(f\"  {name}: {count} bikes\")\n",
    "print(f\"\\nAnomaly Detection:\")\n",
    "print(f\"  Anomalous bikes: {bike_features['is_anomaly'].sum()}\")\n",
    "print(f\"\\nHealth Status:\")\n",
    "for cat in ['\ud83d\udfe2 Stable', '\ud83d\udfe1 Warning', '\ud83d\udd34 Critical']:\n",
    "    count = (bike_features['health_category'] == cat).sum()\n",
    "    pct = count / len(bike_features) * 100\n",
    "    print(f\"  {cat}: {count} bikes ({pct:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}