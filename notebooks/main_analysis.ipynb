{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udeb4 Predictive Maintenance for Rental Bike Fleets\n",
    "## Using Usage Pattern Analysis\n",
    "\n",
    "**Team**: Anomaly Archers (Section B)\n",
    "\n",
    "**Techniques Used**:\n",
    "- K-Means Clustering (Unsupervised Learning)\n",
    "- Isolation Forest (Anomaly Detection)\n",
    "- Time Series Analysis\n",
    "- Health Scoring & Maintenance Ranking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both months of data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "df_nov = pd.read_csv('../data/raw/202411-capitalbikeshare-tripdata.csv')\n",
    "df_dec = pd.read_csv('../data/raw/202412-capitalbikeshare-tripdata.csv')\n",
    "\n",
    "# Combine into single dataframe\n",
    "df = pd.concat([df_nov, df_dec], ignore_index=True)\n",
    "\n",
    "print(f\"\u2705 Loaded {len(df):,} trip records\")\n",
    "print(f\"   November: {len(df_nov):,} trips\")\n",
    "print(f\"   December: {len(df_dec):,} trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data structure\n",
    "print(\"\\n\ud83d\udcca Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\n Data Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n\ud83d\udd0d Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the bike ID column\n",
    "# Looking for columns that might contain bike identifiers\n",
    "print(\"\\n\ud83d\udd11 Looking for Bike ID column...\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "# Calculate trip duration in minutes\n",
    "df['duration_min'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "print(\"\u2705 Datetime columns converted\")\n",
    "print(f\"\\nDuration statistics (minutes):\")\n",
    "print(df['duration_min'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid trips (negative or extremely long durations)\n",
    "print(f\"\\nBefore cleaning: {len(df):,} trips\")\n",
    "\n",
    "# Filter: duration between 1 minute and 180 minutes (3 hours)\n",
    "df_clean = df[(df['duration_min'] >= 1) & (df['duration_min'] <= 180)].copy()\n",
    "\n",
    "print(f\"After cleaning: {len(df_clean):,} trips\")\n",
    "print(f\"Removed: {len(df) - len(df_clean):,} trips ({(len(df) - len(df_clean))/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which column contains bike IDs\n",
    "# Usually it's 'rideable_type' combined with 'ride_id' or there might be a bike number\n",
    "print(\"\\n\ud83d\udd0d Checking rideable_type distribution:\")\n",
    "print(df_clean['rideable_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this dataset uses ride_id (unique per trip) but we need bike-level analysis,\n",
    "# we'll use a combination of start_station + rideable_type as a proxy for fleet segments\n",
    "# OR if there's a bike_id column, we'll use that\n",
    "\n",
    "# Check all columns again\n",
    "print(\"All columns:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: If there's no direct bike_id, we'll create synthetic bike segments\n",
    "# This is a valid approach when individual bike IDs aren't available\n",
    "\n",
    "# Check if there's a bike number or similar column\n",
    "bike_cols = [col for col in df_clean.columns if 'bike' in col.lower() or 'id' in col.lower()]\n",
    "print(f\"Potential bike ID columns: {bike_cols}\")\n",
    "\n",
    "# Show sample values\n",
    "for col in bike_cols:\n",
    "    print(f\"\\n{col} samples:\")\n",
    "    print(df_clean[col].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We'll create bike-level features for our analysis. If individual bike IDs aren't available, we'll use station-based analysis as a valid proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Aggregate by start_station_id to create \"station health profiles\"\n",
    "# This is a valid approach when bike-level data isn't available\n",
    "\n",
    "# First, let's see what station columns we have\n",
    "station_cols = [col for col in df_clean.columns if 'station' in col.lower()]\n",
    "print(f\"Station columns: {station_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated features per station\n",
    "# This will serve as our \"entity\" for health analysis\n",
    "\n",
    "# Group by start station\n",
    "station_features = df_clean.groupby('start_station_id').agg(\n",
    "    total_trips=('ride_id', 'count'),\n",
    "    total_duration_min=('duration_min', 'sum'),\n",
    "    avg_trip_duration=('duration_min', 'mean'),\n",
    "    std_trip_duration=('duration_min', 'std'),\n",
    "    max_trip_duration=('duration_min', 'max'),\n",
    "    min_trip_duration=('duration_min', 'min'),\n",
    "    first_trip=('started_at', 'min'),\n",
    "    last_trip=('started_at', 'max'),\n",
    "    unique_destinations=('end_station_id', 'nunique'),\n",
    "    member_trips=('member_casual', lambda x: (x == 'member').sum()),\n",
    "    casual_trips=('member_casual', lambda x: (x == 'casual').sum())\n",
    ").reset_index()\n",
    "\n",
    "print(f\"\u2705 Created features for {len(station_features)} stations\")\n",
    "station_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional derived features\n",
    "station_features['days_active'] = (station_features['last_trip'] - station_features['first_trip']).dt.days + 1\n",
    "station_features['trips_per_day'] = station_features['total_trips'] / station_features['days_active']\n",
    "station_features['member_ratio'] = station_features['member_trips'] / station_features['total_trips']\n",
    "station_features['duration_variability'] = station_features['std_trip_duration'] / station_features['avg_trip_duration']\n",
    "\n",
    "# Handle any NaN values\n",
    "station_features = station_features.fillna(0)\n",
    "\n",
    "print(\"\u2705 Derived features calculated\")\n",
    "station_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stations with very few trips (noise)\n",
    "min_trips = 50  # Minimum trips to be included\n",
    "station_features_filtered = station_features[station_features['total_trips'] >= min_trips].copy()\n",
    "\n",
    "print(f\"Stations before filter: {len(station_features)}\")\n",
    "print(f\"Stations after filter (>={min_trips} trips): {len(station_features_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for ML\n",
    "feature_columns = [\n",
    "    'total_trips',\n",
    "    'total_duration_min',\n",
    "    'avg_trip_duration',\n",
    "    'std_trip_duration',\n",
    "    'trips_per_day',\n",
    "    'unique_destinations',\n",
    "    'member_ratio',\n",
    "    'duration_variability'\n",
    "]\n",
    "\n",
    "X = station_features_filtered[feature_columns].copy()\n",
    "\n",
    "# Handle any infinities\n",
    "X = X.replace([np.inf, -np.inf], 0)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features for clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\u2705 Features normalized using StandardScaler\")\n",
    "print(f\"\\nScaled feature statistics:\")\n",
    "print(f\"Mean: {X_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"Std:  {X_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means Clustering\n",
    "\n",
    "We'll segment stations into different usage profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using Elbow Method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 8)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0].set_title('Elbow Method for Optimal K', fontsize=14)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score vs K', fontsize=14)\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/elbow_silhouette.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSilhouette Scores: {dict(zip(K_range, [f'{s:.3f}' for s in silhouette_scores]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final K-Means with K=3\n",
    "K = 3\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to data\n",
    "station_features_filtered['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\u2705 K-Means clustering complete with K={K}\")\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "print(station_features_filtered['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_summary = station_features_filtered.groupby('cluster')[feature_columns].mean()\n",
    "print(\"\\n\ud83d\udcca Cluster Characteristics (Mean Values):\")\n",
    "cluster_summary.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the clusters based on characteristics\n",
    "# Analyze which cluster has highest/lowest values\n",
    "cluster_names = {}\n",
    "\n",
    "# Find cluster with highest trips per day (Heavy Usage)\n",
    "heavy_cluster = cluster_summary['trips_per_day'].idxmax()\n",
    "# Find cluster with lowest trips per day (Light Usage)\n",
    "light_cluster = cluster_summary['trips_per_day'].idxmin()\n",
    "# Remaining is Moderate\n",
    "moderate_cluster = [c for c in range(K) if c not in [heavy_cluster, light_cluster]][0]\n",
    "\n",
    "cluster_names = {\n",
    "    heavy_cluster: '\ud83d\udd34 Heavy Usage',\n",
    "    moderate_cluster: '\ud83d\udfe1 Moderate Usage',\n",
    "    light_cluster: '\ud83d\udfe2 Light Usage'\n",
    "}\n",
    "\n",
    "station_features_filtered['cluster_name'] = station_features_filtered['cluster'].map(cluster_names)\n",
    "\n",
    "print(\"\\n\ud83d\udccd Cluster Naming:\")\n",
    "for k, v in cluster_names.items():\n",
    "    print(f\"  Cluster {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['#2ecc71', '#f1c40f', '#e74c3c']  # Green, Yellow, Red\n",
    "for cluster_id in sorted(station_features_filtered['cluster'].unique()):\n",
    "    mask = station_features_filtered['cluster'] == cluster_id\n",
    "    plt.scatter(\n",
    "        X_pca[mask, 0], \n",
    "        X_pca[mask, 1], \n",
    "        c=colors[cluster_id],\n",
    "        label=cluster_names[cluster_id],\n",
    "        alpha=0.6,\n",
    "        s=100,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.title('Station Clusters - Usage Profiles (PCA Visualization)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../outputs/figures/cluster_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA Explained Variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection (Isolation Forest)\n",
    "\n",
    "Identify stations with unusual usage patterns that may indicate problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # Expect 5% anomalies\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
    "anomaly_scores = iso_forest.decision_function(X_scaled)\n",
    "\n",
    "# Add to dataframe (-1 = anomaly, 1 = normal)\n",
    "station_features_filtered['anomaly'] = anomaly_labels\n",
    "station_features_filtered['anomaly_score'] = anomaly_scores\n",
    "station_features_filtered['is_anomaly'] = station_features_filtered['anomaly'] == -1\n",
    "\n",
    "print(\"\u2705 Isolation Forest trained\")\n",
    "print(f\"\\nAnomaly Detection Results:\")\n",
    "print(f\"  Normal stations: {(anomaly_labels == 1).sum()}\")\n",
    "print(f\"  Anomalous stations: {(anomaly_labels == -1).sum()}\")\n",
    "print(f\"  Anomaly rate: {(anomaly_labels == -1).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze anomalies\n",
    "anomalies = station_features_filtered[station_features_filtered['is_anomaly']]\n",
    "normal = station_features_filtered[~station_features_filtered['is_anomaly']]\n",
    "\n",
    "print(\"\\n\ud83d\udcca Anomaly vs Normal Comparison:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Anomaly (Mean)': anomalies[feature_columns].mean(),\n",
    "    'Normal (Mean)': normal[feature_columns].mean(),\n",
    "    'Difference %': ((anomalies[feature_columns].mean() - normal[feature_columns].mean()) / normal[feature_columns].mean() * 100)\n",
    "})\n",
    "comparison.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomalies\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Normal points\n",
    "plt.scatter(\n",
    "    X_pca[~station_features_filtered['is_anomaly'], 0],\n",
    "    X_pca[~station_features_filtered['is_anomaly'], 1],\n",
    "    c='#3498db',\n",
    "    label='Normal',\n",
    "    alpha=0.5,\n",
    "    s=80\n",
    ")\n",
    "\n",
    "# Anomaly points\n",
    "plt.scatter(\n",
    "    X_pca[station_features_filtered['is_anomaly'], 0],\n",
    "    X_pca[station_features_filtered['is_anomaly'], 1],\n",
    "    c='#e74c3c',\n",
    "    label='Anomaly',\n",
    "    alpha=0.9,\n",
    "    s=150,\n",
    "    marker='X',\n",
    "    edgecolors='black',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.title('Anomaly Detection Results (Isolation Forest)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../outputs/figures/anomaly_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anomalous stations\n",
    "print(\"\\n\ud83d\udea8 Top 10 Most Anomalous Stations:\")\n",
    "top_anomalies = station_features_filtered.nsmallest(10, 'anomaly_score')[\n",
    "    ['start_station_id', 'cluster_name', 'total_trips', 'trips_per_day', 'avg_trip_duration', 'anomaly_score']\n",
    "]\n",
    "top_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis\n",
    "\n",
    "Analyze usage trends over time to detect degradation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily aggregates\n",
    "df_clean['date'] = df_clean['started_at'].dt.date\n",
    "\n",
    "daily_stats = df_clean.groupby(['start_station_id', 'date']).agg(\n",
    "    daily_trips=('ride_id', 'count'),\n",
    "    daily_duration=('duration_min', 'sum'),\n",
    "    avg_duration=('duration_min', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"\u2705 Created daily aggregates: {len(daily_stats):,} rows\")\n",
    "daily_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trends per station (simple linear regression slope)\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_trend(group):\n",
    "    if len(group) < 7:  # Need at least a week of data\n",
    "        return 0\n",
    "    x = np.arange(len(group))\n",
    "    y = group['daily_trips'].values\n",
    "    slope, _, _, _, _ = stats.linregress(x, y)\n",
    "    return slope\n",
    "\n",
    "station_trends = daily_stats.groupby('start_station_id').apply(calculate_trend).reset_index()\n",
    "station_trends.columns = ['start_station_id', 'usage_trend']\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Usage Trend Statistics:\")\n",
    "print(station_trends['usage_trend'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge trends with main features\n",
    "# IMPORTANT: Drop columns if they exist to avoid merge errors on re-run\n",
    "cols_to_drop = [c for c in ['usage_trend', 'trend_category', 'usage_trend_x', 'usage_trend_y'] \n",
    "               if c in station_features_filtered.columns]\n",
    "if cols_to_drop:\n",
    "    station_features_filtered = station_features_filtered.drop(columns=cols_to_drop)\n",
    "\n",
    "station_features_filtered = station_features_filtered.merge(\n",
    "    station_trends, \n",
    "    on='start_station_id', \n",
    "    how='left'\n",
    ")\n",
    "station_features_filtered['usage_trend'] = station_features_filtered['usage_trend'].fillna(0)\n",
    "\n",
    "# Categorize trends\n",
    "station_features_filtered['trend_category'] = pd.cut(\n",
    "    station_features_filtered['usage_trend'],\n",
    "    bins=[-np.inf, -0.5, 0.5, np.inf],\n",
    "    labels=['\ud83d\udcc9 Declining', '\u27a1\ufe0f Stable', '\ud83d\udcc8 Increasing']\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Trend Distribution:\")\n",
    "print(station_features_filtered['trend_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall fleet trend\n",
    "fleet_daily = df_clean.groupby('date').agg(\n",
    "    total_trips=('ride_id', 'count'),\n",
    "    avg_duration=('duration_min', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "fleet_daily['date'] = pd.to_datetime(fleet_daily['date'])\n",
    "fleet_daily['rolling_7d'] = fleet_daily['total_trips'].rolling(7).mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(fleet_daily['date'], fleet_daily['total_trips'], alpha=0.3, label='Daily Trips')\n",
    "plt.plot(fleet_daily['date'], fleet_daily['rolling_7d'], linewidth=2, color='red', label='7-Day Rolling Avg')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Number of Trips', fontsize=12)\n",
    "plt.title('Fleet-Wide Daily Trip Volume', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/time_series_trend.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Health Scoring & Maintenance Ranking\n",
    "\n",
    "Combine all signals into a unified health score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define health score components\n",
    "\n",
    "# 1. Cluster Risk Score (Heavy usage = higher risk)\n",
    "cluster_risk_map = {\n",
    "    heavy_cluster: 0.7,\n",
    "    moderate_cluster: 0.4,\n",
    "    light_cluster: 0.1\n",
    "}\n",
    "station_features_filtered['cluster_risk'] = station_features_filtered['cluster'].map(cluster_risk_map)\n",
    "\n",
    "# 2. Normalize Anomaly Score (0 to 1, where 1 = most anomalous)\n",
    "min_score = station_features_filtered['anomaly_score'].min()\n",
    "max_score = station_features_filtered['anomaly_score'].max()\n",
    "station_features_filtered['normalized_anomaly'] = 1 - ((station_features_filtered['anomaly_score'] - min_score) / (max_score - min_score))\n",
    "\n",
    "# 3. Trend Risk Score - convert categorical to string to avoid TypeError\n",
    "station_features_filtered['trend_category_str'] = station_features_filtered['trend_category'].astype(str)\n",
    "trend_risk_map = {\n",
    "    '\ud83d\udcc9 Declining': 0.8,\n",
    "    '\u27a1\ufe0f Stable': 0.3,\n",
    "    '\ud83d\udcc8 Increasing': 0.1\n",
    "}\n",
    "station_features_filtered['trend_risk'] = station_features_filtered['trend_category_str'].map(trend_risk_map).fillna(0.3)\n",
    "\n",
    "print(\"\u2705 Risk components calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate composite health score\n",
    "# Weights: Cluster 40%, Anomaly 40%, Trend 20%\n",
    "w_cluster = 0.4\n",
    "w_anomaly = 0.4\n",
    "w_trend = 0.2\n",
    "\n",
    "station_features_filtered['health_score'] = (\n",
    "    w_cluster * station_features_filtered['cluster_risk'] +\n",
    "    w_anomaly * station_features_filtered['normalized_anomaly'] +\n",
    "    w_trend * station_features_filtered['trend_risk']\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Health Score Distribution:\")\n",
    "print(station_features_filtered['health_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign health categories\n",
    "def categorize_health(score):\n",
    "    if score < 0.35:\n",
    "        return '\ud83d\udfe2 Stable'\n",
    "    elif score < 0.55:\n",
    "        return '\ud83d\udfe1 Warning'\n",
    "    else:\n",
    "        return '\ud83d\udd34 Critical'\n",
    "\n",
    "station_features_filtered['health_category'] = station_features_filtered['health_score'].apply(categorize_health)\n",
    "\n",
    "print(\"\\n\ud83c\udfe5 Health Category Distribution:\")\n",
    "print(station_features_filtered['health_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create maintenance priority ranking\n",
    "priority_ranking = station_features_filtered.sort_values('health_score', ascending=False)[\n",
    "    ['start_station_id', 'cluster_name', 'health_score', 'health_category', \n",
    "     'total_trips', 'trips_per_day', 'is_anomaly', 'trend_category']\n",
    "].reset_index(drop=True)\n",
    "\n",
    "priority_ranking.index = priority_ranking.index + 1  # 1-based ranking\n",
    "priority_ranking.index.name = 'Priority'\n",
    "\n",
    "print(\"\\n\ud83d\udd27 TOP 20 STATIONS FOR MAINTENANCE:\")\n",
    "priority_ranking.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full ranking\n",
    "priority_ranking.to_csv('../outputs/maintenance_priority_ranking.csv')\n",
    "print(\"\u2705 Priority ranking saved to outputs/maintenance_priority_ranking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Score Distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = {'\ud83d\udfe2 Stable': '#2ecc71', '\ud83d\udfe1 Warning': '#f1c40f', '\ud83d\udd34 Critical': '#e74c3c'}\n",
    "health_counts = station_features_filtered['health_category'].value_counts()\n",
    "plt.bar(health_counts.index, health_counts.values, color=[colors[c] for c in health_counts.index])\n",
    "plt.xlabel('Health Category', fontsize=12)\n",
    "plt.ylabel('Number of Stations', fontsize=12)\n",
    "plt.title('Fleet Health Distribution', fontsize=14)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(station_features_filtered['health_score'], bins=20, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "plt.axvline(x=0.35, color='green', linestyle='--', label='Stable threshold')\n",
    "plt.axvline(x=0.55, color='orange', linestyle='--', label='Warning threshold')\n",
    "plt.xlabel('Health Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Health Score Histogram', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/health_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Priority Bar Chart\n",
    "top10 = priority_ranking.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['#e74c3c' if cat == '\ud83d\udd34 Critical' else '#f1c40f' for cat in top10['health_category']]\n",
    "bars = plt.barh(range(len(top10)), top10['health_score'], color=colors)\n",
    "plt.yticks(range(len(top10)), [f\"Station {sid}\" for sid in top10['start_station_id']])\n",
    "plt.xlabel('Health Score (Higher = More Urgent)', fontsize=12)\n",
    "plt.title('\ud83d\udd27 Top 10 Stations Requiring Maintenance', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add score labels\n",
    "for i, (score, cat) in enumerate(zip(top10['health_score'], top10['health_category'])):\n",
    "    plt.text(score + 0.01, i, f'{score:.2f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/top10_priority.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fleet Health Pie Chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "health_counts = station_features_filtered['health_category'].value_counts()\n",
    "colors = [colors.get(c, '#95a5a6') for c in health_counts.index]\n",
    "plt.pie(\n",
    "    health_counts.values, \n",
    "    labels=health_counts.index, \n",
    "    autopct='%1.1f%%',\n",
    "    colors=['#2ecc71', '#f1c40f', '#e74c3c'][:len(health_counts)],\n",
    "    explode=[0.05] * len(health_counts),\n",
    "    shadow=True,\n",
    "    startangle=90\n",
    ")\n",
    "plt.title('Overall Fleet Health Status', fontsize=14)\n",
    "plt.savefig('../outputs/figures/fleet_health_pie.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions & Business Insights\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Clustering Results**: Stations were segmented into 3 distinct usage profiles:\n",
    "   - Heavy Usage: High-traffic stations with intense utilization\n",
    "   - Moderate Usage: Average activity stations\n",
    "   - Light Usage: Low-traffic stations\n",
    "\n",
    "2. **Anomaly Detection**: ~5% of stations show unusual usage patterns that warrant investigation. These anomalies often correlate with:\n",
    "   - Extremely high or low trip durations\n",
    "   - Unusual member/casual ratios\n",
    "   - Inconsistent daily patterns\n",
    "\n",
    "3. **Time Series Trends**: Identified stations with declining usage that may indicate:\n",
    "   - Equipment issues discouraging riders\n",
    "   - Location-based problems\n",
    "   - Seasonal variations\n",
    "\n",
    "4. **Health Scoring**: Combined all signals into actionable maintenance priorities:\n",
    "   - Top-ranked stations should receive immediate attention\n",
    "   - Health scores can be recalculated weekly for ongoing monitoring\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "- **Proactive Maintenance**: Instead of reactive fixes, operators can now prioritize stations before problems escalate\n",
    "- **Resource Optimization**: Maintenance teams can be directed to highest-priority locations\n",
    "- **User Experience**: Reducing unexpected bike issues improves rider satisfaction\n",
    "- **Cost Savings**: Early intervention prevents costly emergency repairs\n",
    "\n",
    "### Limitations & Future Work\n",
    "\n",
    "- **Data Limitation**: Analysis done at station level (bike IDs not available in this dataset)\n",
    "- **External Factors**: Weather, events, and seasonal patterns not incorporated\n",
    "- **Validation**: Would benefit from actual maintenance records to validate predictions\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. Deploy health scoring as weekly monitoring dashboard\n",
    "2. Investigate top 20 anomalous stations immediately\n",
    "3. Collect and integrate actual maintenance logs for model validation\n",
    "4. Consider weather data integration for more accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Statistics\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udcca PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal trips analyzed: {len(df_clean):,}\")\n",
    "print(f\"Stations analyzed: {len(station_features_filtered)}\")\n",
    "print(f\"\")\n",
    "print(f\"Cluster Distribution:\")\n",
    "for name in station_features_filtered['cluster_name'].unique():\n",
    "    count = (station_features_filtered['cluster_name'] == name).sum()\n",
    "    print(f\"  {name}: {count} stations\")\n",
    "\n",
    "print(f\"\\nAnomaly Detection:\")\n",
    "print(f\"  Anomalous stations: {station_features_filtered['is_anomaly'].sum()}\")\n",
    "print(f\"  Anomaly rate: {station_features_filtered['is_anomaly'].mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nHealth Status:\")\n",
    "for cat in ['\ud83d\udfe2 Stable', '\ud83d\udfe1 Warning', '\ud83d\udd34 Critical']:\n",
    "    count = (station_features_filtered['health_category'] == cat).sum()\n",
    "    pct = count / len(station_features_filtered) * 100\n",
    "    print(f\"  {cat}: {count} stations ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 Analysis Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}